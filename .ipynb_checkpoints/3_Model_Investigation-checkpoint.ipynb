{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An NLP workshop - Categorizing tweets into relevant or non-relevant\n",
    "#### adapted from https://github.com/hundredblocks/concrete_NLP_tutorial.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Overview\n",
    "\n",
    "In this workshop you will combine a few different NLP techniques to classify tweets. You will:\n",
    "- clean up your input data\n",
    "- transform your text data into numerical vectors (because machine learning algorithms need numbers as inputs!)\n",
    "- use those vectors as input to machine learning classifiers\n",
    "\n",
    "We will present you with the code for three different **word embeddings** (i.e. ways to turn text into vectors of numbers) and two different **classification algorithms**. It will be your job to select an embedding and a model, train it with the data provided (you might want to perform some additional **cleansing** of the data beforehand) and then test the accuracy of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import all the libraries we will need upfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in our cleansed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = pd.read_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split\n",
    "First lets split our data into a training set and a test set. Later on we will 'fit' our model using the training set and then use the model to predict the labels for the test set and compare the predicted labels against the actual test set labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed to get consistent results for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = clean_questions[\"text\"]\n",
    "y = clean_questions[\"class_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(\"Training set: %d samples\" % len(X_train))\n",
    "print(\"Test set: %d samples\" % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Let's define a function to help us assess accuracy of our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define a function that plots a *Confusion Matrix* which helps us see our false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    #plt.yticks(tick_marks, classes, fontsize=20)  # for some reason this is causing problems\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to the Machine Learning\n",
    "Now that our data is clean and prepared, let's dive in to the machine learning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter embeddings\n",
    "Machine Learning on images can use raw pixels as inputs. Fraud detection algorithms can use customer features. What can NLP use?\n",
    " \n",
    "A natural way to represent text for computers is to encode each character individually, this seems quite inadequate to represent and understand language. Our goal is to first create a useful embedding for each sentence (or tweet) in our dataset, and then use these embeddings to accurately predict the relevant category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some syntactic sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "Dataset = collections.namedtuple('Dataset','X y')\n",
    "Embedding = collections.namedtuple('Embedding', 'train test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Counts\n",
    "The simplest approach we can start with is to use a bag of words model. A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "bow_train = Dataset(count_vectorizer.fit_transform(X_train), y_train)\n",
    "bow_test  = Dataset(count_vectorizer.transform(X_test), y_test)\n",
    "bow = Embedding(bow_train, bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow.train.X.shape)\n",
    "print(bow.test.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Bag of Words\n",
    "Let's try a slightly more subtle approach. On top of our bag of words model, we use a TF-IDF (Term Frequency, Inverse Document Frequency) which means weighing words by how frequent they are in our dataset, discounting words that are too frequent, as they just add to the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "\n",
    "tfidf_train = Dataset(tfidf_vectorizer.fit_transform(X_train), y_train)\n",
    "tfidf_test  = Dataset(tfidf_vectorizer.transform(X_test), y_test)\n",
    "tfidf = Embedding(tfidf_train, tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf.train.X.shape)\n",
    "print(tfidf.test.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Capturing semantic meaning\n",
    "Our first models have managed to pick up on high signal words. However, it is unlikely that we will have a training set containing all relevant words. To solve this problem, we need to capture the semantic meaning of words. Meaning we need to understand that words like 'good' and 'positive' are closer than apricot and 'continent'.\n",
    "\n",
    "Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"~/Downloads/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, clean_questions)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(embeddings, y, test_size=0.2, random_state=40)\n",
    "\n",
    "w2v_train = Dataset(X_train_w2v, y_train_w2v)\n",
    "w2v_test  = Dataset(X_test_w2v, y_test_w2v)\n",
    "w2v = Embedding(w2v_train, w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"({len(w2v.train.X)}, {len(w2v.train.X[0])})\")\n",
    "print(f\"({len(w2v.test.X)}, {len(w2v.test.X[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression classifier\n",
    "Starting with a logistic regression is a good idea. It is simple, often gets the job done, and is easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr_classifier():\n",
    "    return LogisticRegression(C=30.0, class_weight='balanced', solver='lbfgs', \n",
    "                              multi_class='multinomial', max_iter=500, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine classifier\n",
    "Common alternative to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def lsvm_classifier(): \n",
    "    return LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def nb_classifier():\n",
    "    return MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train some models!\n",
    "Select your combination of embedding and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tfidf                  # bow | tfidf | w2v\n",
    "classifier = lsvm_classifier()     # lr_classifier | lsvm_classifier | nb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(*embedding.train)\n",
    "\n",
    "y_predict = classifier.predict(embedding.test.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Let's start by looking at some metrics to see if our classifier performed well at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(embedding.test.y, y_predict)\n",
    "print(\"accuracy = {:2.2%}, precision = {:2.2%}, recall ={:2.2%}, f1 = {:2.2%}\".format(accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(embedding.test.y, y_predict)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster','Unsure'], normalize=False, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold cross-validation\n",
    "Evaluating a Machine Learning model can be quite tricky. So far, we have split the data set into training and testing sets and used the training set to train the model and testing set to test the model. We then evaluated the model performance based on an error metric to determine the accuracy of the model. This method, however, is not very reliable as the accuracy obtained for one test set can be very different to the accuracy obtained for a different test set. K-fold Cross Validation(CV) provides a solution to this problem by dividing the data into folds and ensuring that each fold is used as a testing set at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# We use a pipeline to ensure that the vectorizer fit and transform \n",
    "# happens separately for each fold\n",
    "clf = make_pipeline(TfidfVectorizer(), lsvm_classifier())\n",
    "data_X = X_train\n",
    "data_y = y_train\n",
    "\n",
    "# Use full dataset\n",
    "scores = cross_validate(clf, data_X, data_y, scoring=['accuracy'], cv=5, return_train_score=True)\n",
    "print(\"Validation accuracy - Mean:{:2.2%}, Std:{:2.2%}\".format(scores['test_accuracy'].mean(), scores['test_accuracy'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV\n",
    "Classifiers often have various parameters (often called hyper-parameters) which can affect the accuracy of the model. You can combine cross-validation with a hyper-parameter search to find the optimal set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for each parameter specify a range of values you want to test\n",
    "C_range    = 2. ** np.arange(-5, 5, step=1)\n",
    "\n",
    "# build a dictionary of parameters\n",
    "parameters = [{'logisticregression__C': C_range}]\n",
    "\n",
    "# pass the dictionary to GridSearchCV specifying that it's the LogisticRegression\n",
    "# and indicating the number of cross validation folds\n",
    "clf = make_pipeline(TfidfVectorizer(), LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=500))\n",
    "grid = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# display the results\n",
    "bestC = grid.best_params_['logisticregression__C']\n",
    "print (f\"The best parameters are: cost = {bestC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters and check the accuracy metrics\n",
    "clf = LogisticRegression(C=1.0, solver='lbfgs', multi_class='auto', max_iter=500)\n",
    "clf.fit(*tfidf.train)\n",
    "accuracy, precision, recall, f1 = get_metrics(tfidf.test.y, clf.predict(tfidf.test.X))\n",
    "print(\"accuracy = {:2.2%}, precision = {:2.2%}, recall = {:2.2%}, f1 = {:2.2%}\".format(accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using grid search for the SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Test datasets\n",
    "\n",
    "Generally, best practice is to have Training, Validation (sometimes called Dev) and Test Datasets.\n",
    "\n",
    "- Training set: A set of examples used for learning, that is to fit the parameters of the classifier.\n",
    "- Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network.\n",
    "- Test set: A set of examples used only to assess the performance of a fully-specified classifier.\n",
    "\n",
    "For a more detailed discussion, see: https://machinelearningmastery.com/difference-test-validation-datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlpw)",
   "language": "python",
   "name": "nlpw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
